# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zVygyXjmsfxbmCTNFVWauxJZtEPU64qu
"""

import requests
import csv
from bs4 import BeautifulSoup as bs
words_to_search = input('What to search ')
words=''
for i in words_to_search.split(' '):
  words=words+i+'+'

base_url = f'https://hh.ru/search/vacancy?area=1&st=searchVacancy&text={words}'

headers = {'accept':'*/*','user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0'}


def parser(base_url,headers):
  jobs = []
  urls = []
  session = requests.Session()
  request = session.get(base_url, headers=headers)
  urls.append(base_url)
  if request.status_code == 200:
    soup = bs(request.content,'lxml')
    try:
      pagination = soup.find_all('a', attrs={'data-qa': 'pager-page'})
      count = int(pagination[-1].text) 
      for i in range(count):
          url = f'https://hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=data&page={i}' 
          if url not in urls:
            urls.append(url)
    except:
      pass 
    for url in urls:
      request = session.get(url, headers=headers)
      soup = bs(request.content,'lxml')
      divs = soup.find_all('div',attrs={'class': 'vacancy-serp-item '})
      for div in divs:
        
        try:
          title = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-title'}).text
          href = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-title'})['href']
          company = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-employer'}).text
          responsibility = div.find('div', attrs={'data-qa': 'vacancy-serp__vacancy_snippet_responsibility'}).text
          requirement = div.find('div', attrs={'data-qa': 'vacancy-serp__vacancy_snippet_requirement'}).text              
          jobs.append({'title': title,
                      'href': href,
                      'company': company,
                      'responsibility': responsibility,
                      'requirement': requirement
              })
        except:
          pass
        
  return jobs          
def writer(jobs):
   with open ('parsed_jobs.csv', 'w') as file:
     a_pen = csv.writer(file)
     a_pen.writerow(('Название вакансии', 'Url', 'Название Компании', 'Обязанности', 'Требования'))
     for job in jobs:
       a_pen.writerow((job['title'],job['href'],job['company'],job['responsibility'],job['requirement']))
       
jobs = parser(base_url,headers)
writer(jobs)





